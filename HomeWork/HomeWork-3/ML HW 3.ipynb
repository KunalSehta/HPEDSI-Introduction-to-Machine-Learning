{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file ...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1. Load the dataset in the file named .csv\n",
    "# Pandas for data handling\n",
    "import pandas # https://pandas.pydata.org/\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "print('Loading data from file ...')  # Now let's load the data\n",
    "dataset = pandas.read_csv('C:/Users/kseht/Downloads/winequality-white.csv') # default is header=infer, change if column names are not in first row\n",
    "print('done \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset - in table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#produce at least one table and one graph that summarize the dataset statistics\n",
    "# Let's look at a numerical summary table\n",
    "\n",
    "print('Summary of the dataset - in table')   \n",
    "display(dataset.describe(include='all'))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading list of problem variables X and Y...\n",
      "done \n",
      "\n",
      "done with setting aside data for testing\n"
     ]
    }
   ],
   "source": [
    "# 2. predicting the FlowPattern value based on the values of the variables named Vsl, Vsg, and Ang.\n",
    "print('Reading list of problem variables X and Y...')\n",
    "X_name = [ 'fixed acidity', 'volatile acidity', 'citric acid','residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide','density', 'pH', 'sulphates', 'alcohol'] # columns to focus on as predictors\n",
    "X = dataset[X_name]   # only keep these columns as features     \n",
    "y_name = 'quality'     # column to focus on as target\n",
    "y = dataset[y_name]   # only keep this column as label \n",
    "print('done \\n')\n",
    "# Split data into training and testing datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "test_pct = 0.20   # reserve 20% of the data points for testing performance\n",
    "seed = 7          # setting the seed allows for repeatability\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_pct, random_state=seed)\n",
    "print('done with setting aside data for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9.18167246\n",
      "Iteration 2, loss = 2.73336411\n",
      "Iteration 3, loss = 1.79512200\n",
      "Iteration 4, loss = 1.50056844\n",
      "Iteration 5, loss = 1.41707547\n",
      "Iteration 6, loss = 1.40280341\n",
      "Iteration 7, loss = 1.32391247\n",
      "Iteration 8, loss = 1.38788032\n",
      "Iteration 9, loss = 1.32004596\n",
      "Iteration 10, loss = 1.30353504\n",
      "Training set score: 0.471159\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Task 1  at least three different combinations of MLPClassifier architecture choices (e.g., number of layers, # of neurons per layer, activation function).\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Design the classifier neural network\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,50,), # two hidden layers with 50 neurons each\n",
    "                    activation = 'identity',  # ReLU is the default option\n",
    "                    batch_size = 'auto',\n",
    "                    #shuffle = True,\n",
    "                    #solver='sgd',  # default is Adam\n",
    "                    #learning_rate = 'adaptive',\n",
    "                    alpha=1e-4,  # regulariztion parameter, set to default=0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                    #learning_rate_init=.1 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                    max_iter=10,  # number of epochs, default=200\n",
    "                    random_state=42,\n",
    "                    verbose=10,\n",
    "                    )\n",
    "\n",
    "# Train the classifier\n",
    "# NOTE: this example won't converge because our max_iter choice is too few epochs \n",
    "# (otherwise it will take too long for a live demo), \n",
    "# so we catch the warning and ignore it here\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00        39\n",
      "           5       0.00      0.00      0.00       280\n",
      "           6       0.47      1.00      0.64       456\n",
      "           7       0.00      0.00      0.00       162\n",
      "           8       0.00      0.00      0.00        36\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.47       980\n",
      "   macro avg       0.07      0.14      0.09       980\n",
      "weighted avg       0.22      0.47      0.30       980\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseht\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test / 255.    # rescale the test data using the same scaler as for the training set\n",
    "\n",
    "y_predicted = mlp.predict(X_test)   # use the trained classifier to predict on the test set\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report(y_test, y_predicted))  # compare predictions with ground truth\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 18.45747543\n",
      "Iteration 2, loss = 13.64465524\n",
      "Iteration 3, loss = 16.18634415\n",
      "Iteration 4, loss = 8.41254806\n",
      "Iteration 5, loss = 3.06391914\n",
      "Iteration 6, loss = 1.42093311\n",
      "Iteration 7, loss = 1.26819119\n",
      "Iteration 8, loss = 1.25907709\n",
      "Iteration 9, loss = 1.24614482\n",
      "Iteration 10, loss = 1.23279830\n",
      "Training set score: 0.437468\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Task 2 the performance impact of varying at least three different combinations of optimizer parameter values\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Design the classifier neural network\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,50,), # two hidden layers with 50 neurons each\n",
    "                    activation = 'identity',  # ReLU is the default option\n",
    "                    batch_size = 'auto',\n",
    "                    shuffle = True,\n",
    "                    solver='sgd',  # default is Adam\n",
    "                    learning_rate = 'adaptive',\n",
    "                    alpha=1e-4,  # regulariztion parameter, set to default=0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                    #learning_rate_init=.1 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                    max_iter=10,  # number of epochs, default=200\n",
    "                    random_state=42,\n",
    "                    verbose=10,\n",
    "                    )\n",
    "\n",
    "# Train the classifier\n",
    "# NOTE: this example won't converge because our max_iter choice is too few epochs \n",
    "# (otherwise it will take too long for a live demo), \n",
    "# so we catch the warning and ignore it here\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00        39\n",
      "           5       0.29      1.00      0.44       280\n",
      "           6       0.00      0.00      0.00       456\n",
      "           7       0.00      0.00      0.00       162\n",
      "           8       0.00      0.00      0.00        36\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.29       980\n",
      "   macro avg       0.04      0.14      0.06       980\n",
      "weighted avg       0.08      0.29      0.13       980\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseht\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test / 255.    # rescale the test data using the same scaler as for the training set\n",
    "\n",
    "y_predicted = mlp.predict(X_test)   # use the trained classifier to predict on the test set\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report(y_test, y_predicted))  # compare predictions with ground truth\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3:\n",
    "#Test the performance of the best MLPClassifier from Steps 1 and 2, using scoring methods of your choice. Discuss in detail your results.\n",
    "# In task 2 : for 10 iterations the values are jus the half as the values fir task3\n",
    "# The accuracey value for task 2 is greater than the task 3 as the loss percentage is less in the task to as compared to task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseht\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found on development set for Decision Tree:\n",
      "{'criterion': 'gini', 'max_depth': 5}\n",
      "Decision tree has maximum depth 5.\n",
      "f1_score is\n",
      "0.040530397798348765\n"
     ]
    }
   ],
   "source": [
    "#Task 4: Train and tune a different classifier that is not a neural network; compare the MLPClassifier test results from Step 3 to that other classifier. Discuss in detail your results. \n",
    "# implemented the random forest classifier\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "# First, let’s import all of the modules, functions and objects we are going to use in this tutorial.\n",
    "\n",
    "# Pandas for data handling\n",
    "import pandas # https://pandas.pydata.org/\n",
    "\n",
    "# NumPy for numerical computing\n",
    "import numpy # https://numpy.org/\n",
    "\n",
    "# MatPlotLib for visualization\n",
    "import matplotlib.pyplot as pl  # https://matplotlib.org/\n",
    "\n",
    "# assessment\n",
    "from sklearn import model_selection # for model comparisons\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Decision Tree\n",
    "print('Tuning model...')\n",
    "selected_model = DecisionTreeClassifier()\n",
    "hyperparameters = {'max_depth':[5, 6, 7], 'criterion':['gini', 'entropy'] }\n",
    "clf = GridSearchCV(selected_model, hyperparameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters found on development set for Decision Tree:\")\n",
    "print(clf.best_params_)\n",
    "tuned_model_DT = clf.best_estimator_\n",
    "\n",
    "print(f'Decision tree has maximum depth {tuned_model_DT.tree_.max_depth}.')\n",
    "y_pred = tuned_model_DT.predict(X_test)\n",
    "print( 'f1_score is')\n",
    "print( f1_score(y_test, y_pred, average='macro') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseht\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found on development set for Random Forest:\n",
      "{'criterion': 'gini', 'max_depth': 5, 'n_estimators': 100}\n",
      "f1_score is\n",
      "0.0634920634920635\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print('Tuning model...')\n",
    "selected_model = RandomForestClassifier()\n",
    "hyperparameters = {'max_depth':[3, 4, 5], 'criterion':['gini', 'entropy'], 'n_estimators':[10, 50, 100] }\n",
    "clf = GridSearchCV(selected_model, hyperparameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters found on development set for Random Forest:\")\n",
    "print(clf.best_params_)\n",
    "tuned_model_RF = clf.best_estimator_\n",
    "\n",
    "y_pred = tuned_model_RF.predict(X_test)\n",
    "print( 'f1_score is')\n",
    "print( f1_score(y_test, y_pred, average='macro') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented the random forest classifier and found that the accuracey for the random forest classifier is more than that of the MPL classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
